<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>线性注意力背后的视角转换 | The Paradigm Shift Behind Linear Attention</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['\\(', '\\)']],
                displayMath: [['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            }
        };
    </script>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei", "Helvetica Neue", Helvetica, Arial, sans-serif;
            line-height: 1.8;
            color: #333;
            background: #fafafa;
            padding: 20px;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            background: white;
            padding: 40px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            border-radius: 8px;
        }

        .header {
            text-align: center;
            margin-bottom: 40px;
            padding-bottom: 30px;
            border-bottom: 2px solid #e0e0e0;
        }

        h1 {
            font-size: 2.2em;
            margin-bottom: 15px;
            color: #1a1a1a;
            font-weight: 600;
        }

        .author {
            font-size: 1.1em;
            color: #666;
            margin-bottom: 8px;
        }

        .date {
            font-size: 0.95em;
            color: #999;
        }

        .language-switcher {
            display: flex;
            justify-content: center;
            gap: 10px;
            margin-bottom: 30px;
        }

        .lang-btn {
            padding: 10px 20px;
            background: #f0f0f0;
            border: 2px solid #ddd;
            border-radius: 5px;
            cursor: pointer;
            font-size: 1em;
            transition: all 0.3s;
        }

        .lang-btn:hover {
            background: #e0e0e0;
        }

        .lang-btn.active {
            background: #4a90e2;
            color: white;
            border-color: #4a90e2;
        }

        .content {
            display: none;
        }

        .content.active {
            display: block;
        }

        .content h2 {
            font-size: 1.6em;
            margin-top: 40px;
            margin-bottom: 20px;
            color: #2c3e50;
            padding-bottom: 10px;
            border-bottom: 1px solid #e0e0e0;
        }

        .content h3 {
            font-size: 1.3em;
            margin-top: 30px;
            margin-bottom: 15px;
            color: #34495e;
        }

        .content h4 {
            font-size: 1.1em;
            margin-top: 25px;
            margin-bottom: 12px;
            color: #555;
        }

        .content p {
            margin-bottom: 18px;
            text-align: justify;
        }

        .content ul, .content ol {
            margin: 20px 0;
            padding-left: 30px;
        }

        .content li {
            margin-bottom: 12px;
        }

        .content strong {
            color: #2c3e50;
            font-weight: 600;
        }

        .content em {
            font-style: italic;
            color: #555;
        }

        .formula {
            background: #f8f9fa;
            padding: 15px;
            border-left: 4px solid #4a90e2;
            margin: 20px 0;
            overflow-x: auto;
            text-align: center;
        }
        
        .formula .MathJax {
            font-size: 1.1em;
        }

        .references {
            margin-top: 50px;
            padding-top: 30px;
            border-top: 2px solid #e0e0e0;
        }

        .references h2 {
            font-size: 1.4em;
            margin-bottom: 20px;
        }

        .references ol {
            list-style: none;
            counter-reset: ref-counter;
        }

        .references li {
            counter-increment: ref-counter;
            margin-bottom: 15px;
            padding-left: 30px;
            position: relative;
        }

        .references li::before {
            content: "[" counter(ref-counter) "]";
            position: absolute;
            left: 0;
            font-weight: bold;
            color: #4a90e2;
        }

        .divider {
            height: 1px;
            background: #e0e0e0;
            margin: 30px 0;
        }

        @media (max-width: 768px) {
            .container {
                padding: 20px;
            }

            h1 {
                font-size: 1.6em;
            }

            .content h2 {
                font-size: 1.3em;
            }

            .content h3 {
                font-size: 1.1em;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 id="title-zh">线性注意力背后的视角转换</h1>
            <h1 id="title-en" style="display: none;">The Paradigm Shift Behind Linear Attention</h1>
            <div class="author" id="author">Zhichen Zhao</div>
            <div class="date" id="date-zh">Jan 1, 2026</div>
            <div class="date" id="date-en" style="display: none;">Dec 31, 2025</div>
        </div>

        <div class="language-switcher">
            <button class="lang-btn active" onclick="switchLanguage('zh')">中文</button>
            <button class="lang-btn" onclick="switchLanguage('en')">English</button>
        </div>

        <div id="content-zh" class="content active">
            <p>我最近阅读了 Google Research 的两篇论文【1】、【2】，并回溯了最早提出线性注意力机制的论文【3】。我在阅读过程中再一次体会到视角转换如何为一个理论框架赋予新的想象空间。如果你没有经历过这种转换，你会低估它的深远意义。联想量子力学的发展历史：</p>

            <div class="formula">\[
            \text{矩阵力学 / 波动力学} \Rightarrow \text{量子场论 / 正则量子化} \Rightarrow \text{路径积分}
            \]</div>

            <p>每次变革不仅仅是用一套新的数学公式重写既有知识，而是用一套全新的视角和语言审视理论。在这个意义上，（数学）语言不仅是思维的边界，它内嵌了思维的空间结构。</p>

            <h2>线性注意力机制打开了什么空间？</h2>

            <p>传统的注意力机制本质上做了两件事情：</p>
            <ol>
                <li>将输入 \(X\) 映射到三个矩阵：Key，Value，Query</li>
                <li>计算输出：\(Y = \mathrm{softmax}(QK^\top)V\)（为了表述简便，忽略归一化与掩码矩阵）</li>
            </ol>

            <p>要实现全局注意力，就要一个包含位置信息的全局矩阵来编码注意力矩阵 \(QK^\top\)，与之伴随的是空间平方的复杂度 \(O(n^2)\)，和超长上下文的工程难题。若将注意力核从 softmax 放宽为可分解的核函数（或用随机特征/显式特征映射近似 softmax kernel），则可将注意力写成：</p>

            <div class="formula">\[
            \mathrm{Attn}(Q,K,V) \approx \phi(Q)\big(\phi(K)^\top V\big)
            \]</div>

            <p>在自回归（causal）情形下，令</p>
            <div class="formula">\[
            S_t = \sum_{i \le t} \phi(k_i) v_i^\top, \qquad z_t = \sum_{i \le t} \phi(k_i)
            \]</div>

            <p>则输出可写成：</p>
            <div class="formula">\[
            o_t = \frac{\phi(q_t)^\top S_t}{\phi(q_t)^\top z_t}
            \]</div>

            <p>其中 S_t 是一个随 token 流递推更新、与上下文长度无关的中间态。分母项 z_t 的归一化对于数值稳定和长度增长时的尺度漂移至关重要，这也是线性注意力处理灾难性遗忘的关键机制。Katharopoulos 等人指出，这种递推实现使线性注意力在因果场景下呈现出明确的 RNN 结构：注意力从"扫描全部 KV 列表"转为"更新固定大小的隐状态并进行读出"【3】。</p>

            <p><strong>这种重述带来了新的视角。</strong>中间状态 S_t 描述了一个随着输入流更新的记忆状态，也就是 RNN 里的隐状态。这是一个典型的关联记忆模型（associative memory），它编码了从 Key 到 Value 的映射关系。面对一个新的 Query，输出行为就是从 S_t 中提取出与 Q 关联的记忆。关联记忆模型正是 2024 年 Hopfield 和 Hinton 获得物理学诺贝尔奖的工作的基础<sup><a href="#note1">*</a></sup>。启发它的赫布理论以及衍生出的突触可塑性依然是今天主流神经生物学的基础理论。</p>

            <h2>波函数类比</h2>

            <p>中间状态 S_t 的引入，让我想起量子力学历史上矩阵力学与波动力学之争。深受马赫主义影响的海森堡认为物理理论应当避免引入缺乏操作定义的量，比如电子在轨道上的精确位置和经典动量等，只有轨道之间的跃迁信息是可观测量，应当用矩阵来描述。与此同时，德布罗意从光的波粒二相性出发，认为一切物质都具有波动性，需要用波函数描述这种物质波。注意：在波恩的概率诠释之前，德布罗意认为这是真实存在的实体波（就像电磁波一样），而不仅仅是便于计算的隐喻。虽然两个理论很快被证明在数学上是等价的——从本体论层面上，波函数从物质实体降格为量子态在位置表象下的表征（representation）——但是波函数相较抽象晦涩的矩阵拥有几何直觉与可视化的优势。它在帮助我们想象（而非理解）理论的可能性上仍有价值。</p>

            <p><strong>在我看来，线性注意力的中间状态 S_t 并不是额外引入了新的"本体论对象"，更像是一次表象切换：</strong>把原本隐含在一次性注意力计算中的线性映射（从 key 空间到 value 空间的关联）显式化为一个可递推更新的算符/状态。就像波函数作为量子态在特定基下的表示，提供了强烈的几何直觉与可视化入口；S_t 的显式化同样把"记忆是什么、如何写入/遗忘、如何分层"变成了可以被直接设计与分析的对象。</p>

            <h2>从静态到动态："惊讶度"的物理学</h2>

            <p>在此基础上，【1】提出了一个更深刻的视角转换。它将记忆更新视作一个在线学习（online learning）的动力学过程。在 Titans 的视角里，长期记忆的"写入"可以被理解为对某个 associative memory loss 的在线优化：每来一段新数据，就在记忆参数上走一步（或少数几步）梯度更新。更关键的是，更新强度并非恒定，而是由"惊讶度（surprise）"调制：违反模型预期的数据更"值得被记住"。Titans 用梯度作为 surprise 的度量，并结合衰减机制实现记忆管理；作者还指出该机制与带动量与 weight decay 的小批量梯度法存在等价联系。</p>

            <p>我们对反过来的表述都很熟悉。传统的机器学习就是基于这样一套逻辑：预设一个目标函数，通过已标注训练集（在线或离线），训练出一个最佳模型。此处，目标函数是具象的（预测与标注的误差），模型参数是抽象的。而在"从关联记忆到在线学习"的图景转换里，模型参数是具象的（记忆），参数的动力学过程也是具象的（记忆更新），我们要反推出一个目标函数，是相对抽象的。这个目标函数被诠释为"惊讶度"：人们对习以为常的现象不会加以过多关注，意外事件更容易被记住，即触发记忆更新机制。</p>

            <p>任何一个接受过高等物理学训练的人都会对这个逆向思维非常敏感，因为它是分析力学的核心思想。基于速度、加速度、力等矢量概念的牛顿力学表述是动力学过程，它用"动力因"描绘了一整套机械宇宙图景；但是当代力学体系所依赖的是一系列标量：哈密顿量、拉格朗日量、作用量、路径积分、配分函数。这些标量不仅完美兼容动力学表述，还可以简洁地描述系统遵循的对称性，深刻揭示对称与守恒的关系，优雅地处理边界条件和自由度。它们极其晦涩抽象，但是在数学上如此优越，足以让人们抛弃一个更直观的动力学图景。</p>

            <p>一旦为线性注意力机制赋予了关联记忆和在线学习的诠释，我们就可以泛化以上两个步骤。【4】将泛化自由度归为四类：</p>
            <ol>
                <li>记忆框架，用神经网络取代矩阵编码 \(K \to V\)，获得更高的自由度同时降低过拟合风险。</li>
                <li>优化目标：设计损失函数。</li>
                <li>遗忘门：记忆与遗忘一体两面，学习因子和遗忘因子本身可以是可学习的参数。</li>
                <li>优化器：从损失函数到记忆更新的优化机制，Gradient Descent, SGD, Adam…</li>
            </ol>

            <p>【1】用"顺行性遗忘症（anterograde amnesia）"这个比喻来描述今天的大模型：它拥有在某个时间节点前（预训练语料库截止时间）的所有记忆。在运行时，它也拥有执行该任务所需要的所有短期记忆（上下文）。但是，一旦任务结束，这些上下文被立刻抛弃，无法形成长期记忆。Titans 提出的 test-time memorization / online learning 可以突破这个藩篱，因为它不必保留所有上下文，只需要通过流经的短期记忆形成长期记忆（也就是 \(S_t\)）即可。与之相对的基础模型（提供了从输入 \(X\) 到 \(QKV\) 的映射矩阵）除了提供对世界的基本理解以外，还担负着"更有效地形成长期记忆"的职责。</p>

            <p>需要区分的是，<strong>Titans 的在线学习</strong>与<strong>Test-Time Training (TTT) layers</strong>【7】是不同的概念：TTT 特指 Sun 等人提出的方法，他们将 hidden state 直接做成一个小模型，每个 token/mini-batch 做一步自监督学习更新；而 Titans 关注的是长期记忆模块的训练，用 surprise 和衰减机制来管理记忆。</p>

            <p>对于这一点，【2】认为【1】的做法不够精致。从更新频率的视角来看，【1】本质上提供了二元更新频率，即：</p>
            <ul>
                <li>外层循环，f=0，基础模型，静态，不更新。</li>
                <li>内层循环，f=1，以token流为单位，每流经一个token就更新一次记忆状态。</li>
            </ul>

            <p>Nested Learning 进一步把"更新频率"提升为一个新的设计轴：对系统中每个组件定义其 update rate（权重多久更新一次），据此将嵌套/并行的内部优化问题排序成 levels，并由此得到"连续谱"的记忆系统（CMS）：不同模块在不同频率上压缩与固化其各自的 context flow。Titans 可以被视为只有两级频率的特例。</p>

            <p>而人的记忆更像是一个层级结构：从原初的感官信息（sensory registeration）开始，部分进入工作记忆（working memory）并被编码为具体的情境记忆（episodic memory）。随着系统整合的发生，这些记忆经历"去情境化"（decontextualization），最终提炼为稳固的语义知识（semantic knowledge），并内化为深层的认知图式（cognitive schemas）与世界观。（当然实际情况比这个单一链条复杂得多，比如我们今天的大部分抽象知识都在语义知识空间里直接生成，并没有对应的情境记忆。）这不仅仅是一个抽象的认识论模型，它更有着神经生物的基础，在大脑中与高频（gamma）到低频（delta）的神经振荡相对应。这也是当前神经科学研究的核心范式之一。</p>

            <h2>时间的洋葱：嵌套学习与重整化群</h2>

            <p>【2】设计了这样一套层级的记忆与学习模型框架。如同深度神经网络和stacked transformer在空间维度实现信息的层级抽象，记忆行为在时间维度实现层级抽象。当然它同时也是空间维度的，因为无论【2】还是大脑，不同频率的记忆与学习发生在不同的区域（从后脑到前额叶）。【2】提出了一个洋葱式的内嵌结构，最外层直面原初信息流，更新最快；最内层是基础模型，不更新；中间每一层都在某个特定的频率上对内层提炼与浓缩。【1】可以被看作只有两层的洋葱。</p>

            <p><strong>"层级结构"再一次触动了我的物理学神经。</strong>从某种程度上来说，物理学是研究"尺度"的学科。很幸运，我们并没有生活在一个缩放对称的物理世界（不然世界会变成一幅宏大、氤氲、永恒流动的分形抽象画）。指导整个物理世界运作的，是几个有量纲的物理常数，其中最基本的就是普朗克时间和普朗克空间。另一方面，物理学探索跨越时空的永恒性与普世性，那么时空尺度就必然成为任何理论都无法绕过的属性。重整化群（Renormalization Group，RG）便是探索物理理论与尺度关系的数学工具。这个令人困惑的名字来自量子场论。"重整化"最初是用来消除量子电动力学中臭名昭著的紫外发散问题的"奇技淫巧"，之后人们发现它背后蕴含某种深刻的不变性，被凝聚态物理借用后发扬光大，成为一门关于尺度的普世方法（Wilsonian RG）。"群"意味着它处理的尺度变换符合结合律。对这部分物理感兴趣的可以读我的科普书【6】。</p>

            <p>重整化群是一套普世方法，无论对象是量子电动力学、永磁体还是沙堆，无论尺度变换是连续的还是离散的，它都用同一个框架去审视系统。在这套框架里最核心的概念是不动点（fixed point），或严格来说是非平庸的不动点（称之为"临界点"）。临界点针对参数空间而言，它指系统在一套特定的参数下（比如磁铁所处的温度和外磁场强度）不随时间或空间的尺度（比如磁铁原子的自旋块，block spin）的变化而变化。临界点并不是一个"点"，而是一切符合尺度不变性的参数自空间。在深度学习模型里，空间尺度坐标是神经网络的深度（从下到上）；在【2】里，时间尺度坐标是洋葱的层数（从外到里）。（其实重整化流的图像对扩散模型最直观，因为它的时间和空间尺度变化是绑定的。）</p>

            <p>我们可以尝试用"接近某种非平庸不动点/不变流形"来组织现象，检验其是否能解释泛化与遗忘的权衡。作为类比示意，我们可以想象两类平庸的不动点：<strong>红外不动点</strong>——过拟合，模型仅仅是一本字典，没有泛化能力；<strong>紫外不动点</strong>——白噪音 / 灾难性遗忘，模型记不住任何东西，至少无法可靠地记住任何东西。而在非平庸的临界点上，模型收敛到了某个固定的参数流形上。它有效地冲刷掉了原初信息中的噪音，保留了记忆和认知中最深层的结构。注意，临界点性质不足以判断模型好坏，但它提供了一套归类方法（普世类，Universality Class），将模型的微扰自由度分为相关算符（relevant operator）和无关算符（irrelevant operator）。在同一个普世类下，所有无关算符都会被积分掉（integrated out），而相关算符则不断被强化。我想提出的是，对于记忆模型而言，特别是在个性化AI的语境下，我们真正关心的不是那些强相关算符，而是在两者边界上的<strong>边缘算符（marginal operator）</strong>。强相关算符对应的是基础模型中已经习得的通用知识（语法、基础知识、普世价值观），它对记忆模型而言并不重要；真正重要的是从"我"的记忆中提取的关于"我"的偏好、思维方式和价值观，这些信息最有可能编码在边缘算符中。令人期待的是，重整化群不仅为这种概念区分提供了可量化定义的蓝图，还提供了一个系统方案来计算模型在相关算符附近的状态方程（equation of state）。它不是第一性原理层面上的动力学方程（equation of dynamics），但它提供了丰富的模型库，来探索普世类流形的动态平衡关系。</p>

            <p>以上是我最近的阅读体验。如果你看完一头雾水，这不怪你（当然也不怪我）。首先我自己并没有想清楚许多细节，特别是如何将【2】里的自由度映射到RG框架里；其次我也没有面向普通读者写作（和我的科普书不同）。我想象中的读者是兼具物理学和AI研究背景的科学家——这在今天的AI圈不在少数。物理学对于AI的滋养远未结束，尤其在关于"尺度"的研究上——这远不是一个"Scaling Law"可以涵盖的丰富宝藏。</p>

            <div class="references">
                <h2>参考</h2>
                <ol>
                    <li><a href="https://arxiv.org/abs/2501.00663" target="_blank">Titans: Learning to Memorize at Test Time</a> (Dec 2024)</li>
                    <li><a href="https://research.google/blog/introducing-nested-learning-a-new-ml-paradigm-for-continual-learning/" target="_blank">Nested Learning: The Illusion of Deep Learning Architecture</a> (Nov 2025)</li>
                    <li><a href="https://arxiv.org/abs/2006.16236" target="_blank">Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention</a> (Aug 2020)</li>
                    <li><a href="https://arxiv.org/abs/2504.13173" target="_blank">It's All Connected: A Journey Through Test-Time Memorization, Attentional Bias, Retention, and Online Optimization</a> (Apr 2025)</li>
                    <li><a href="https://spaces.ac.cn/archives/11033" target="_blank">线性注意力简史：从模仿、创新到反哺</a> By 苏剑林，2025-06-20</li>
                    <li><a href="https://www.douban.com/doubanapp/dispatch/book/36819634" target="_blank">什么是物理？用物理学的视角看世界（下）：近代物理篇</a>，2024</li>
                    <li><a href="https://arxiv.org/abs/2407.04620" target="_blank">Learning to (Learn at Test Time): RNNs with Expressive Hidden States</a> (2024)</li>
                </ol>
                <p id="note1" style="margin-top: 20px; font-size: 0.9em; color: #666;"><sup>*</sup> 2024年诺贝尔物理学奖授予 John J. Hopfield 与 Geoffrey Hinton，表彰他们在机器学习/神经网络领域的基础性贡献。详见 <a href="https://www.nobelprize.org/prizes/physics/2024/summary/" target="_blank">Nobel Prize 官方页面</a>。</p>
            </div>
        </div>

        <div id="content-en" class="content">
            <p>I recently read two Google Research papers [1, 2] and revisited the seminal paper that first introduced the linear attention mechanism [3]. The process reminded me once again how a shift in perspective can endow a theoretical framework with entirely new imaginative space. If you haven't experienced such a shift, it is easy to underestimate its profound significance. Consider the history of quantum mechanics:</p>

            <div class="formula">\[
            \text{Matrix Mechanics / Wave Mechanics} \rightarrow \text{Quantum Field Theory / Canonical Quantization} \rightarrow \text{Path Integral}
            \]</div>

            <p>Each revolution was not merely rewriting existing knowledge with new formulas; it was a re-examination of theory through a completely new lens and language. In this sense, mathematical language is not just the boundary of thought; it embeds the very spatial structure of thinking.</p>

            <h2>What space does Linear Attention open up?</h2>

            <p>Traditional attention mechanisms essentially perform two tasks:</p>
            <ol>
                <li>Map the input \(X\) to three matrices: Key, Value, and Query.</li>
                <li>Compute the output: \(Y = \mathrm{softmax}(QK^\top)V\) (ignoring normalization and masking for brevity).</li>
            </ol>

            <p>To achieve global attention, one requires a global matrix containing positional information to encode the attention matrix \(QK^\top\). This comes with quadratic spatial complexity \(O(n^2)\) and the engineering challenges of handling ultra-long contexts.</p>

            <p>If we relax the Softmax requirement and use a decomposable kernel function (or approximate the softmax kernel with random/explicit feature maps), we can write attention as:</p>

            <div class="formula">\[
            \mathrm{Attn}(Q,K,V) \approx \phi(Q)\big(\phi(K)^\top V\big)
            \]</div>

            <p>In the autoregressive (causal) setting, let</p>
            <div class="formula">\[
            S_t = \sum_{i \le t} \phi(k_i) v_i^\top, \qquad z_t = \sum_{i \le t} \phi(k_i)
            \]</div>

            <p>then the output can be written as:</p>
            <div class="formula">\[
            o_t = \frac{\phi(q_t)^\top S_t}{\phi(q_t)^\top z_t}
            \]</div>

            <p>where S_t is an intermediate state that updates recursively with the token stream and is independent of context length. The normalization term z_t is crucial for numerical stability and scale drift as length grows—this is a key mechanism for linear attention to handle catastrophic forgetting. Katharopoulos et al. point out that this recursive implementation reveals an explicit RNN structure in causal linear attention: attention transforms from "scanning the entire KV list" to "updating a fixed-size hidden state and reading out" [3].</p>

            <p><strong>This reformulation brings a new perspective.</strong> The intermediate state S_t describes a memory state that updates with the input stream—equivalent to the hidden state in an RNN. This is a typical <strong>associative memory model</strong>, encoding the mapping from Key to Value. Faced with a new Query, the output behavior is simply extracting the memory associated with Q from S_t. Associative memory models are the foundation of the work for which Hopfield and Hinton were awarded the 2024 Nobel Prize in Physics<sup><a href="#note1-en">*</a></sup>. The Hebbian theory that inspired it, along with the derived concept of synaptic plasticity, remains a cornerstone of modern neurobiology.</p>

            <h2>The Wave Function Analogy</h2>

            <p>The introduction of the intermediate state S_t reminds me of the historical debate between Matrix Mechanics and Wave Mechanics. Heisenberg, deeply influenced by Machian positivism, believed physical theories should avoid quantities lacking operational definitions—such as the precise position and classical momentum of an electron in orbit. Only the transition information between orbits was observable and thus should be described by matrices.</p>

            <p>Meanwhile, de Broglie, starting from the wave-particle duality of light, proposed that all matter has a wave nature requiring a wave function description. Note that before Born's probability interpretation, de Broglie believed this was a real, physical wave (like an electromagnetic wave), not just a calculation metaphor. Although the two theories were soon proven mathematically equivalent—and ontologically, the wave function was "downgraded" from a material entity to a representation of a quantum state in the position basis—the wave function still possessed geometric intuition and visualization advantages over the abstract, obscure matrices. It remains valuable for imagining (rather than just calculating) theoretical possibilities.</p>

            <p><strong>In my view, the intermediate state S_t in linear attention is not an additional "ontological object," but rather a representation switch:</strong> it makes explicit the linear mapping (the association from key space to value space) that was previously implicit in the one-shot attention computation, turning it into a recursively updatable operator/state. Just as the wave function, as a representation of a quantum state in a specific basis, provides strong geometric intuition and visualization; the explicitization of S_t similarly turns "what memory is, how to write/forget, how to layer" into objects that can be directly designed and analyzed.</p>

            <h2>From Static to Dynamic: The Physics of "Surprise"</h2>

            <p>Building on this, [1] proposes an even deeper perspective shift. In Titans' view, the "writing" of long-term memory can be understood as online optimization of an associative memory loss: for each new piece of data, take one (or a few) gradient update steps on the memory parameters. More crucially, the update strength is not constant but modulated by "surprise": data that violates model expectations is more "worth remembering." Titans use gradients as a measure of surprise and combine it with decay mechanisms for memory management; the authors also point out that this mechanism has equivalent connections to mini-batch gradient descent with momentum and weight decay.</p>

            <p>We are familiar with the inverse formulation. Traditional machine learning is based on pre-setting an objective function and training an optimal model via a labeled dataset (online or offline). There, the objective is concrete (prediction error), and the model parameters are abstract. However, in the shift from "associative memory" to "online learning," the model parameters are concrete (the memory), and the dynamics are concrete (the update rule), while we must infer the objective function, which is relatively abstract. This objective is interpreted as "Surprise": we tend to ignore habitual phenomena, but unexpected events are more likely to be remembered, triggering the memory update mechanism.</p>

            <p>Anyone with training in advanced physics will be sensitive to this inverse thinking—it is the core of <strong>Analytical Mechanics</strong>. Newton's formulation, based on vectors like velocity, acceleration, and force, is a dynamical process painting a mechanical universe via "efficient causes." However, modern mechanics relies on scalars: the Hamiltonian, Lagrangian, Action, Path Integral, and Partition Function. These scalars not only perfectly accommodate dynamical descriptions but also elegantly describe symmetries, conservation laws, boundary conditions, and degrees of freedom. They are abstract and obscure, yet their mathematical superiority is sufficient to make us abandon the more intuitive Newtonian picture.</p>

            <p>Once we endow linear attention with the interpretations of associative memory and online learning, we can generalize the steps. [4] categorizes the degrees of freedom into four classes:</p>
            <ol>
                <li><strong>Memory Architecture:</strong> Using neural networks to replace the matrix encoding \(K \to V\), gaining higher degrees of freedom while reducing overfitting risks.</li>
                <li><strong>Objective:</strong> Designing the loss function.</li>
                <li><strong>Forget Gate:</strong> Memory and forgetting are two sides of the same coin; learning and forgetting factors can themselves be learnable parameters.</li>
                <li><strong>Optimizer:</strong> The mechanism from loss function to memory update (GD, SGD, Adam, etc.).</li>
            </ol>

            <p>[1] uses the analogy of <em>anterograde amnesia</em> to describe today's LLMs: they possess all memories prior to a specific cutoff (pre-training), and during runtime, they hold the short-term memory (context) needed for the task. But once the task ends, the context is discarded, never forming long-term memory. Titans' test-time memorization / online learning breaks this barrier because it doesn't need to retain the full context buffer; it only needs to form long-term memory (state \(S_t\)) via the flowing short-term memory. Conversely, the "Foundation Model" (which provides the \(X \to QKV\) mapping) is responsible not just for basic world understanding, but for "learning how to form long-term memories effectively."</p>

            <p>It is important to distinguish <strong>Titans' online learning</strong> from <strong>Test-Time Training (TTT) layers</strong> [7]: TTT specifically refers to Sun et al.'s approach, where they make the hidden state directly into a small model, performing one step of self-supervised learning update per token/mini-batch; whereas Titans focuses on training long-term memory modules, using surprise and decay mechanisms to manage memory.</p>

            <h2>The Onion of Time: Nested Learning and RG</h2>

            <p>Regarding this, [2] critiques [1] as lacking refinement. From the perspective of update frequency, [1] essentially offers a binary frequency:</p>
            <ul>
                <li><strong>Outer Loop (f=0):</strong> Foundation model, static, no update.</li>
                <li><strong>Inner Loop (f=1):</strong> Updates memory state with every token in the stream.</li>
            </ul>

            <p>Nested Learning further elevates "update frequency" as a new design axis: for each component in the system, define its update rate (how often weights update), thereby ordering nested/parallel internal optimization problems into levels, and thus obtaining a "continuous spectrum" memory system (CMS): different modules compress and consolidate their respective context flows at different frequencies. Titans can be seen as a special case with only two frequency levels.</p>

            <p>Human memory, however, resembles a hierarchical structure: starting from raw <strong>sensory registration</strong>, entering <strong>working memory</strong>, encoded into <strong>episodic memory</strong>, and through consolidation, de-contextualized into stable <strong>semantic knowledge</strong> and deep <strong>cognitive schemas</strong>. (Reality is more complex, but this holds conceptually). This hierarchy has neurobiological grounding, corresponding to neural oscillations ranging from high-frequency Gamma to low-frequency Delta waves.</p>

            <p>[2] designs a hierarchical memory and learning framework. Just as Deep Neural Networks and Stacked Transformers achieve hierarchical abstraction in space, memory behavior achieves hierarchical abstraction in <em>time</em>. [2] proposes an onion-like nested structure:</p>
            <ul>
                <li><strong>Outermost Layer:</strong> Faces the raw information flow, fastest update frequency.</li>
                <li><strong>Innermost Layer:</strong> The foundation model, static.</li>
                <li><strong>Middle Layers:</strong> Each extracts and condenses information from the outer layer at a specific frequency.</li>
            </ul>
            <p>([1] can be seen as an onion with only two layers).</p>

            <p><strong>"Hierarchical Structure" triggers my physics instincts again.</strong> Physics is, to some extent, the study of "Scale." We are lucky not to live in a scale-invariant world (otherwise, reality would be a fractal abstract painting). Governing our world are dimensional constants, the most fundamental being Planck time and Planck length.</p>

            <p><strong>Renormalization Group (RG)</strong> is the mathematical tool for exploring the relationship between theory and scale. Originating from QFT to fix UV divergence in QED, it revealed profound invariances and became a universal method in Condensed Matter Physics (Wilsonian RG). "Group" implies the scale transformations satisfy associativity.</p>

            <p>RG applies a unified framework whether the object is QED, a permanent magnet, or a sandpile. The core concept is the <strong>Fixed Point</strong> (specifically, the Critical Point). A critical point refers to a subspace of parameters where the system remains invariant under scale transformations (e.g., the block spin of a magnet). In Deep Learning, the spatial scale coordinate is Network Depth; in [2], the temporal scale coordinate is the Onion Layer (Outer → Inner).</p>

            <p>We can attempt to organize phenomena using "proximity to some non-trivial fixed point/invariant manifold," testing whether it can explain the trade-off between generalization and forgetting. As an illustrative analogy, we can imagine two trivial fixed points:</p>
            <ul>
                <li><strong>IR Fixed Point (Overfitting):</strong> The model is just a dictionary; zero generalization.</li>
                <li><strong>UV Fixed Point (White Noise / Catastrophic Forgetting):</strong> The model remembers nothing reliable.</li>
            </ul>

            <p>At a non-trivial Critical Point, the model converges to a specific parameter manifold. It effectively washes out the noise from raw information while retaining the deepest structures of memory and cognition.</p>

            <p>Critically, RG provides a classification method: <strong>Universality Classes</strong>, dividing degrees of freedom into <strong>Relevant Operators</strong> and <strong>Irrelevant Operators</strong>. Under the same universality class, irrelevant operators are "integrated out," while relevant ones are reinforced.</p>

            <p><strong>I propose a hypothesis:</strong> For memory models, especially in the context of <strong>Personalized AI</strong>, we shouldn't care about the <em>strongly Relevant Operators</em>. These correspond to general knowledge (grammar, logic, universal values) already acquired by the Foundation Model. What truly matters are the <strong>Marginal Operators</strong>—those at the boundary. The information about <em>my</em> preferences, <em>my</em> way of thinking, and <em>my</em> values is most likely encoded in these marginal operators.</p>

            <p>Excitingly, RG provides not just a blueprint for these definitions, but a systematic scheme to calculate the <strong>Equation of State</strong> near relevant operators. It's not a dynamical equation from first principles, but it offers a rich library of models to explore the dynamic equilibrium of universality class manifolds.</p>

            <div class="divider"></div>

            <p><em>The above is my recent reading experience. If you find it confusing, it's not your fault (and I hope not mine). I haven't clarified many details myself, specifically mapping the degrees of freedom in [2] to the RG framework. I envision my readers as scientists with backgrounds in both Physics and AI—a growing demographic today. The nourishment Physics provides to AI is far from over, especially regarding "Scale"—a treasure trove far richer than what "Scaling Laws" currently cover.</em></p>

            <div class="references">
                <h2>References</h2>
                <ol>
                    <li><em><a href="https://arxiv.org/abs/2501.00663" target="_blank">Titans: Learning to Memorize at Test Time</a></em> (Dec 2024)</li>
                    <li><em><a href="https://research.google/blog/introducing-nested-learning-a-new-ml-paradigm-for-continual-learning/" target="_blank">Nested Learning: The Illusion of Deep Learning Architecture</a></em> (Nov 2025)</li>
                    <li><em><a href="https://arxiv.org/abs/2006.16236" target="_blank">Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention</a></em> (Aug 2020)</li>
                    <li><em><a href="https://arxiv.org/abs/2504.13173" target="_blank">It's All Connected: A Journey Through Test-Time Memorization, Attentional Bias, Retention, and Online Optimization</a></em> (Apr 2025)</li>
                    <li><em><a href="https://spaces.ac.cn/archives/11033" target="_blank">A Brief History of Linear Attention</a></em> by Su Jianlin (Jun 2025)</li>
                    <li><em><a href="https://www.douban.com/doubanapp/dispatch/book/36819634" target="_blank">What is Physics? Looking at the World through the Lens of Physics (Part II)</a></em> (2024)</li>
                    <li><em><a href="https://arxiv.org/abs/2407.04620" target="_blank">Learning to (Learn at Test Time): RNNs with Expressive Hidden States</a></em> (2024)</li>
                </ol>
                <p id="note1-en" style="margin-top: 20px; font-size: 0.9em; color: #666;"><sup>*</sup> The 2024 Nobel Prize in Physics was awarded to John J. Hopfield and Geoffrey Hinton for their foundational contributions to machine learning/neural networks. See the <a href="https://www.nobelprize.org/prizes/physics/2024/summary/" target="_blank">official Nobel Prize page</a>.</p>
            </div>
        </div>
    </div>

    <script>
        function switchLanguage(lang) {
            // Update buttons
            const buttons = document.querySelectorAll('.lang-btn');
            buttons.forEach(btn => btn.classList.remove('active'));
            event.target.classList.add('active');

            // Update content
            const zhContent = document.getElementById('content-zh');
            const enContent = document.getElementById('content-en');
            const zhTitle = document.getElementById('title-zh');
            const enTitle = document.getElementById('title-en');
            const zhDate = document.getElementById('date-zh');
            const enDate = document.getElementById('date-en');

            if (lang === 'zh') {
                zhContent.classList.add('active');
                enContent.classList.remove('active');
                zhTitle.style.display = 'block';
                enTitle.style.display = 'none';
                zhDate.style.display = 'block';
                enDate.style.display = 'none';
            } else {
                zhContent.classList.remove('active');
                enContent.classList.add('active');
                zhTitle.style.display = 'none';
                enTitle.style.display = 'block';
                zhDate.style.display = 'none';
                enDate.style.display = 'block';
            }
        }
    </script>
</body>
</html>
